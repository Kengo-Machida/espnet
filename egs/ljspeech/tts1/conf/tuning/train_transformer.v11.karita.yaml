# This is the Transformer TTS configuration
# based on this paper https://speechresearch.github.io/papers/almost_unsup_tts_asr_2019.pdf

# network architecture related
model-module: espnet.nets.pytorch_backend.e2e_tts_transformer:Transformer
embed-dim: 256
eprenet-conv-layers: 0  # one more linear layer w/o non-linear will be added for 0-centor
eprenet-conv-filts: 5
eprenet-conv-chans: 256
dprenet-layers: 2  # one more linear layer w/o non-linear will be added for 0-centor
dprenet-units: 256
adim: 256
aheads: 4
elayers: 4
eunits: 1024
dlayers: 4
dunits: 1024
postnet-layers: 5
postnet-filts: 3
postnet-chans: 256
use-masking: True
bce-pos-weight: 5.0
use-batch-norm: True
use-scaled-pos-enc: True
encoder-normalize-before: True
decoder-normalize-before: True
encoder-concate-after: False
decoder-concate-after: False
reduction-factor: 1

# minibatch related
batch-sort-key: output # shuffle or input or output
batch-frames-out: 12000  # 16 * (870 * 80 + 180 * 35)
                       # batch-size * (max_out * dim_out + max_in * dim_in)
                       # resuling in 15 ~ 81 samples in batch (611 batches per epochs) for ljspeech

# training related
transformer-init: pytorch
transformer-warmup-steps: 4000
transformer-lr: 1.0
initial-encoder-alpha: 1.0
initial-decoder-alpha: 1.0
dropout-rate: 0.1
eprenet-dropout-rate: 0.1
dprenet-dropout-rate: 0.1
postnet-dropout-rate: 0.1
transformer-attn-dropout-rate: 0.0
loss-type: "L2"

# optimization related
opt: noam
accum-grad: 2
grad-clip: 1.0
weight-decay: 1e-6
patience: 0
epochs: 2000  # maybe need around 300k iters
              # 2,000 epochs * 611 batches / 4 accum-grad = 305,500 iters

# other
save-interval-epoch: 10
